{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Python310\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder  \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "USELESS_DATA_BOUNDARY= 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSet(object):\n",
    "    def __init__(self, csv_path):\n",
    "        self.data = pd.read_csv(csv_path)\n",
    "        #print(self.data)\n",
    "        self.processed_data = self.data\n",
    "        #self.missing_value_process()\n",
    "\n",
    "        #self.raw_predictor = self.data.iloc[:,:-1].values\n",
    "        #self.raw_response = self.data.iloc[:,-1].values\n",
    "\n",
    "        self.useless_value_process()\n",
    "        self.missing_value_process()\n",
    "        self.tag_transfer()\n",
    "        self.imputer_process()\n",
    "        self.normalize()\n",
    "\n",
    "    def useless_value_process(self):\n",
    "        col_del = ['examide', 'citoglipton', 'glimepiride-pioglitazone']\n",
    "        self.processed_data.drop(col_del, axis=1, inplace = True)\n",
    "    \n",
    "    def missing_value_process(self):\n",
    "        row_num = len(self.data)\n",
    "        col_num = len(self.data.columns)\n",
    "        cols_to_drop = []\n",
    "        for col in range(col_num):\n",
    "            qm_num = 0\n",
    "            for row in range(row_num):\n",
    "                if str(self.data.iat[row,col]) == \"?\":\n",
    "                    qm_num += 1\n",
    "\n",
    "            #print(qm_num/row_num)\n",
    "            if qm_num/row_num >= USELESS_DATA_BOUNDARY:\n",
    "                #print(\"true\")\n",
    "                cols_to_drop.append(self.data.columns[col])\n",
    "        \n",
    "        self.processed_data.drop(columns=cols_to_drop, axis=1, inplace = True)\n",
    "\n",
    "    def imputer_process(self):\n",
    "        # Replace the question marks with the most frequently appeared value for each feature\n",
    "        imp = SimpleImputer(missing_values= -1, strategy='most_frequent')\n",
    "        imp.fit(self.processed_data)\n",
    "\n",
    "\n",
    "    def tag_transfer(self): \n",
    "\n",
    "        le = LabelEncoder()\n",
    "        #self.processed_data.replace('?', -1, inplace=True)\n",
    "        value_to_keep = \"?\"\n",
    "\n",
    "        # 遍历DataFrame的每一列\n",
    "        for column in self.processed_data.columns:\n",
    "            # 如果列的数据类型是对象（通常意味着它是分类类型）\n",
    "            if self.processed_data[column].dtype == 'object':\n",
    "                # 对列应用 LabelEncoder\n",
    "                values_to_encode = self.processed_data[column] != value_to_keep\n",
    "                #original_column = self.processed_data[column].copy()\n",
    "                # 对非特定值应用 LabelEncoder\n",
    "                encoded_values = le.fit_transform(self.processed_data.loc[values_to_encode, column])\n",
    "                self.processed_data[column] = le.fit_transform(self.processed_data[column])\n",
    "                #self.processed_data[column] = pd.Series(encoded_values, index=self.processed_data.index[values_to_encode]).astype(int)\n",
    "                #self.processed_data.loc[~values_to_encode, column] = original_column[~values_to_encode]\n",
    "                self.processed_data[column].fillna(self.processed_data[column], inplace=True)\n",
    "\n",
    "        #print(self.processed_data)\n",
    "    #I have changed        \n",
    "\n",
    "        #self.processed_data = self.raw_data.dropna()\n",
    "\n",
    "    def normalize(self):\n",
    "\n",
    "        # Normalize data using StandardScaler\n",
    "        scaler = StandardScaler()\n",
    "        self.processed_data = scaler.fit_transform(self.processed_data)\n",
    "\n",
    "        # Print normalized data\n",
    "        print(self.processed_data)\n",
    "\n",
    "    def autoencoder(self):\n",
    "\n",
    "        # Example patient data\n",
    "        # x_train = ...\n",
    "\n",
    "        # Normalize your data\n",
    "        scaler = StandardScaler()\n",
    "        x_train_scaled = scaler.fit_transform(self.data)\n",
    "\n",
    "        # This is the size of our encoded representations\n",
    "        encoding_dim = 20  # Adjust based on your needs\n",
    "\n",
    "        # This is our input placeholder\n",
    "        input_data = Input(shape=(self.data.shape[1],))\n",
    "\n",
    "        # \"encoded\" is the encoded representation of the input\n",
    "        encoded = Dense(encoding_dim, activation='relu')(input_data)\n",
    "\n",
    "        # \"decoded\" is the lossy reconstruction of the input\n",
    "        decoded = Dense(self.data.shape[1], activation='sigmoid')(encoded)\n",
    "\n",
    "        # This model maps an input to its reconstruction\n",
    "        autoencoder = Model(input_data, decoded)\n",
    "\n",
    "        # This model maps an input to its encoded representation\n",
    "        encoder = Model(input_data, encoded)\n",
    "\n",
    "        autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "        # Train the model\n",
    "        autoencoder.fit(x_train_scaled, x_train_scaled, epochs=50, batch_size=256, shuffle=True, validation_split=0.2)\n",
    "\n",
    "        # Use encoder to transform data into lower-dimensional space\n",
    "        encoded_data = encoder.predict(x_train_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = DataSet('diabetic_data_training.csv')\n",
    "test_data = DataSet('diabetic_data_test.csv')\n",
    "\n",
    "#train_data.data\n",
    "train_data.processed_data, test_data.processed_data\n",
    "\n",
    "train_data.processed_data.to_csv(\"test_csv.csv\",sep=';',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[149190, 55629189, 'Caucasian', ..., 'No', 'Ch', 'Yes'],\n",
       "        [64410, 86047875, 'AfricanAmerican', ..., 'No', 'No', 'Yes'],\n",
       "        [500364, 82442376, 'Caucasian', ..., 'No', 'Ch', 'Yes'],\n",
       "        ...,\n",
       "        [443854148, 41088789, 'Caucasian', ..., 'No', 'Ch', 'Yes'],\n",
       "        [443857166, 31693671, 'Caucasian', ..., 'No', 'Ch', 'Yes'],\n",
       "        [443867222, 175429310, 'Caucasian', ..., 'No', 'No', 'No']],\n",
       "       dtype=object),\n",
       " array(['>30', 'NO', 'NO', ..., 'NO', 'NO', 'NO'], dtype=object))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv('diabetic_data_training.csv')\n",
    "test_data = pd.read_csv('diabetic_data_test.csv')\n",
    "\n",
    "predictor = train_data.iloc[:,:-1].values\n",
    "response = train_data.iloc[:,-1].values\n",
    "\n",
    "predictor,response "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
